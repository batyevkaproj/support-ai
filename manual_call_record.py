import os
import sys
import time
from datetime import datetime

import numpy as np
import sounddevice as sd
import resampy
import torch
from faster_whisper import WhisperModel

# ================= CONFIG =================
DEVICE_ID = 67              # Voicemeeter Output (B) device id —É sounddevice
INPUT_RATE = 48000
TARGET_RATE = 16000
CHANNELS = 2
BLOCKSIZE = 1024

MODEL_SIZE = "large-v3"     # —è–∫—â–æ –±—É–¥–µ –≤–∞–∂–∫–æ/–Ω–µ—Å—Ç–∞–±—ñ–ª—å–Ω–æ: "medium"
LANG = "uk"

# –†–æ–∑—É–º–Ω—ñ –ø–æ—Ä–æ–≥–∏
MIN_RMS = 0.008             # –Ω–∏–∂—á–µ ‚Äî –≤–≤–∞–∂–∞—î–º–æ —Ç–∏—à–µ—é/—à—É–º–æ–º
NORM_TARGET = 0.90

# Chunking (–∫—Ä–∏—Ç–∏—á–Ω–æ –¥–ª—è —Å—Ç–∞–±—ñ–ª—å–Ω–æ—Å—Ç—ñ)
CHUNK_SECONDS = 12.0        # 10‚Äì15 —Å–µ–∫ –Ω–∞–π–∫—Ä–∞—â–µ
CHUNK_OVERLAP = 1.0         # –ø–µ—Ä–µ–∫—Ä–∏—Ç—Ç—è, —â–æ–± –Ω–µ —Ä—ñ–∑–∞–ª–æ —Å–ª–æ–≤–∞

# VAD tuning
VAD_MIN_SIL_MS = 900        # –∑–±—ñ–ª—å—à–∏–ª–∏ (–±—É–ª–æ 400) -> –º–µ–Ω—à–µ "–ø–æ —Å–ª–æ–≤—É"
# =========================================

os.makedirs("calls", exist_ok=True)

print("CUDA available:", torch.cuda.is_available())
if torch.cuda.is_available():
    print("GPU:", torch.cuda.get_device_name(0))

print("Loading Whisper model...")
model = WhisperModel(
    MODEL_SIZE,
    device="cuda" if torch.cuda.is_available() else "cpu",
    compute_type="float16" if torch.cuda.is_available() else "int8"
)
print("Model loaded")

recorded_audio = []

def callback(indata, frames, time_info, status):
    recorded_audio.append(indata.copy())

print("\nüéß RECORDING STARTED")
print("üëâ –ó–∞–ø–∏—Å —ñ–¥–µ. –ù–∞—Ç–∏—Å–Ω–∏ Ctrl+C –∫–æ–ª–∏ –¥–∑–≤—ñ–Ω–æ–∫ –∑–∞–≤–µ—Ä—à–∏—Ç—å—Å—è.\n")

try:
    with sd.InputStream(
        device=DEVICE_ID,
        channels=CHANNELS,
        samplerate=INPUT_RATE,
        blocksize=BLOCKSIZE,
        dtype="float32",
        callback=callback
    ):
        while True:
            time.sleep(0.1)
except KeyboardInterrupt:
    print("\nüõë Recording stopped")

# ---------- POST PROCESS ----------
if not recorded_audio:
    print("‚ùå No audio captured")
    sys.exit(0)

print("üîß Processing audio...")

audio_np = np.concatenate(recorded_audio, axis=0)

# Split channels: LEFT=operator, RIGHT=client
operator = audio_np[:, 0].astype(np.float32)
client   = audio_np[:, 1].astype(np.float32)

def rms_level(x: np.ndarray) -> float:
    return float(np.sqrt(np.mean(x * x)) + 1e-12)

def normalize_if_needed(x: np.ndarray) -> np.ndarray:
    # –ù–µ –Ω–æ—Ä–º–∞–ª—ñ–∑—É—î–º–æ "–ø–æ—Ä–æ–∂–Ω–µ—á—É" ‚Äî —Ü–µ –≤–∏–∫–ª–∏–∫–∞—î –≥–∞–ª—é—Ü–∏–Ω–∞—Ü—ñ—ó
    r = rms_level(x)
    if r < MIN_RMS:
        return x
    peak = float(np.max(np.abs(x)) + 1e-9)
    return (x / peak) * NORM_TARGET

def resample_16k(x: np.ndarray) -> np.ndarray:
    if INPUT_RATE == TARGET_RATE:
        return x
    return resampy.resample(x, INPUT_RATE, TARGET_RATE)

def chunk_indices(n_samples: int, sr: int, chunk_sec: float, overlap_sec: float):
    chunk = int(chunk_sec * sr)
    overlap = int(overlap_sec * sr)
    step = max(1, chunk - overlap)
    i = 0
    while i < n_samples:
        j = min(n_samples, i + chunk)
        yield i, j
        if j == n_samples:
            break
        i += step

def transcribe_chunk(audio_16k: np.ndarray):
    # faster-whisper –ø—Ä–∏–π–º–∞—î np.float32 16k mono
    segments, _ = model.transcribe(
        audio_16k,
        language=LANG,

        beam_size=1,
        best_of=1,
        temperature=0.2,

        vad_filter=True,
        vad_parameters=dict(
            min_silence_duration_ms=VAD_MIN_SIL_MS
        ),

        condition_on_previous_text=False,
        no_speech_threshold=0.6,
        compression_ratio_threshold=2.0,
    )
    return segments

def transcribe_channel_chunked(raw_audio: np.ndarray, role: str):
    dialog = []
    total_samples = raw_audio.shape[0]
    total_sec = total_samples / INPUT_RATE
    print(f"üßæ {role}: {total_sec:.1f}s, RMS={rms_level(raw_audio):.4f}")

    for i, j in chunk_indices(total_samples, INPUT_RATE, CHUNK_SECONDS, CHUNK_OVERLAP):
        chunk_raw = raw_audio[i:j]
        r = rms_level(chunk_raw)
        if r < MIN_RMS:
            continue

        chunk_raw = normalize_if_needed(chunk_raw)
        chunk_16k = resample_16k(chunk_raw)

        # offset —É —Å–µ–∫—É–Ω–¥–∞—Ö –≤—ñ–¥ –ø–æ—á–∞—Ç–∫—É –≤—Å—å–æ–≥–æ –∑–∞–ø–∏—Å—É
        base_offset = i / INPUT_RATE

        segments = transcribe_chunk(chunk_16k)
        for seg in segments:
            text = (seg.text or "").strip()
            if not text:
                continue
            # seg.start/seg.end —É –º–µ–∂–∞—Ö —á–∞–Ω–∫–∞ -> –¥–æ–¥–∞—î–º–æ base_offset
            start = float(seg.start) + base_offset
            end = float(seg.end) + base_offset
            dialog.append((start, end, role, text))

    return dialog

# 1) –¢—Ä–∞–Ω—Å–∫—Ä–∏–±—É—î–º–æ –æ–∫—Ä–µ–º–æ –∫–∞–Ω–∞–ª–∏, –∞–ª–µ –ü–û –ß–ê–ù–ö–ê–• + –≥–ª–æ–±–∞–ª—å–Ω–∏–π —á–∞—Å
print("üß† Transcribing OPERATOR (LEFT)...")
dialog = transcribe_channel_chunked(operator, "OPERATOR")

print("üß† Transcribing CLIENT (RIGHT)...")
dialog += transcribe_channel_chunked(client, "CLIENT")

# 2) –°–æ—Ä—Ç—É—î–º–æ –ø–æ –≥–ª–æ–±–∞–ª—å–Ω–æ–º—É —á–∞—Å—É
dialog.sort(key=lambda x: x[0])

# 3) –î–µ–¥—É–ø / –∞–Ω—Ç–∏-–µ—Ö–æ
def normalize_text(t: str) -> str:
    # –¥—É–∂–µ –ø—Ä–æ—Å—Ç–µ –Ω–æ—Ä–º–∞–ª—ñ–∑—É–≤–∞–Ω–Ω—è
    t = t.lower().strip()
    t = " ".join(t.split())
    return t

def postprocess(dialog_items):
    cleaned = []
    last_norm = ""
    last_time = -999.0

    for start, end, role, text in dialog_items:
        nt = normalize_text(text)

        # –ø—Ä–∏–±–∏—Ä–∞—î–º–æ –ø–æ–≤–Ω—ñ –¥—É–±–ª—ñ–∫–∞—Ç–∏ –ø—ñ–¥—Ä—è–¥
        if nt == last_norm and (start - last_time) < 1.5:
            continue

        cleaned.append((start, end, role, text))
        last_norm = nt
        last_time = start

    # –∞–Ω—Ç–∏-–µ—Ö–æ: —è–∫—â–æ –æ–¥–Ω–∞–∫–æ–≤–∞ —Ñ—Ä–∞–∑–∞ –∑‚Äô—è–≤–∏–ª–∞—Å—å –≤ –æ–±–æ—Ö —Ä–æ–ª—è—Ö –º–∞–π–∂–µ –æ–¥–Ω–æ—á–∞—Å–Ω–æ ‚Äî –ª–∏—à–∞—î–º–æ –æ–¥–Ω—É (–≥—É—á–Ω—ñ—à—É –º–∏ –Ω–µ –∑–Ω–∞—î–º–æ, —Ç–æ–∂ –ª–∏—à–∏–º–æ –ø–µ—Ä—à—É)
    final = []
    i = 0
    while i < len(cleaned):
        cur = cleaned[i]
        if i + 1 < len(cleaned):
            nxt = cleaned[i + 1]
            if abs(nxt[0] - cur[0]) < 0.6 and normalize_text(nxt[3]) == normalize_text(cur[3]) and nxt[2] != cur[2]:
                final.append(cur)   # –∑–∞–ª–∏—à–∞—î–º–æ –ø–µ—Ä—à—É
                i += 2
                continue
        final.append(cur)
        i += 1

    return final

dialog = postprocess(dialog)

# 4) –ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è
fname = f"calls/call_{datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}.txt"
with open(fname, "w", encoding="utf-8") as f:
    f.write("====== DIALOG ======\n")
    for start, end, role, text in dialog:
        f.write(f"[{start:07.2f}] {role}: {text}\n")
    f.write("====================\n")

print(f"\nüìù SAVED {fname}")
print("====== DIALOG ======")
for start, end, role, text in dialog:
    print(f"[{start:07.2f}] {role}: {text}")
print("====================")
