import os
import sys
import time
import queue
from datetime import datetime

import numpy as np
import sounddevice as sd
import resampy
import torch
from faster_whisper import WhisperModel

# ================= CONFIG =================
DEVICE_ID = 67                 # Voicemeeter Out B1 (Ñƒ Ñ‚ÐµÐ±Ñ Ð±Ñ‹Ð»Ð¸ Ñ†Ð¸Ñ„Ñ€Ñ‹)
INPUT_RATE = 48000
TARGET_RATE = 16000
CHANNELS = 2
BLOCKSIZE = 512                # Ð¼ÐµÐ½ÑŒÑˆÐµ = Ð¼ÐµÐ½ÑŒÑˆÐµ Ð·Ð°Ð´ÐµÑ€Ð¶ÐºÐ°

# Ð´ÐµÑ‚ÐµÐºÑ‚ Ñ€ÐµÑ‡Ð¸
VOLUME_THRESHOLD = 0.0035      # Ð¿Ð¾Ð´ Ñ‚Ð²Ð¾Ð¸ ÑƒÑ€Ð¾Ð²Ð½Ð¸ 0.01..0.17
SILENCE_TIMEOUT = 0.55         # Ð¼ÐµÐ½ÑŒÑˆÐµ = Ð±Ñ‹ÑÑ‚Ñ€ÐµÐµ Ñ€ÐµÐ°ÐºÑ†Ð¸Ñ
MIN_AUDIO_SECONDS = 0.5        # Ð½Ðµ Ñ€ÐµÐ¶ÐµÐ¼ ÑÐ»Ð¸ÑˆÐºÐ¾Ð¼ ÐºÐ¾Ñ€Ð¾Ñ‚ÐºÐ¾

# Whisper
MODEL_SIZE = "large-v3"          # GPU Ñ‚ÑÐ½ÐµÑ‚; ÐµÑÐ»Ð¸ Ð½Ð°Ð´Ð¾ Ð±Ñ‹ÑÑ‚Ñ€ÐµÐµ -> "small"
LANG = "uk"
PROMPT = "Ð¦Ðµ Ñ‚ÐµÐ»ÐµÑ„Ð¾Ð½Ð½Ð° Ñ€Ð¾Ð·Ð¼Ð¾Ð²Ð° ÑƒÐºÑ€Ð°Ñ—Ð½ÑÑŒÐºÐ¾ÑŽ Ð¼Ð¾Ð²Ð¾ÑŽ."

# Whisper speed/quality knobs
BEAM_SIZE = 1                  # Ð±Ñ‹ÑÑ‚Ñ€ÐµÐµ
VAD_FILTER = False             # Ð¼Ñ‹ ÑÐ°Ð¼Ð¸ Ñ€ÐµÐ¶ÐµÐ¼ Ð¿Ð¾ Ñ‚Ð¸ÑˆÐ¸Ð½Ðµ
CONDITION_ON_PREV = False      # Ð±Ñ‹ÑÑ‚Ñ€ÐµÐµ/ÑÑ‚Ð°Ð±Ð¸Ð»ÑŒÐ½ÐµÐµ Ð´Ð»Ñ ÐºÑƒÑÐ¾Ñ‡ÐºÐ¾Ð²
# =========================================

os.makedirs("calls", exist_ok=True)

# ---------- GPU check ----------
cuda_ok = torch.cuda.is_available()
print("CUDA available:", cuda_ok)
if cuda_ok:
    try:
        print("GPU:", torch.cuda.get_device_name(0))
    except Exception:
        pass

device = "cuda" if cuda_ok else "cpu"
compute_type = "float16" if cuda_ok else "int8"

print("Loading Whisper model...")
model = WhisperModel(
    MODEL_SIZE,
    device=device,
    compute_type=compute_type
)
print(f"Model loaded ({MODEL_SIZE}, device={device}, compute={compute_type})")

audio_queue = queue.Queue(maxsize=200)

current_audio = []
last_voice_time = None
segment_active = False


def rms(x: np.ndarray) -> float:
    # x shape: (frames, channels)
    return float(np.sqrt(np.mean(x ** 2)))


def callback(indata, frames, time_info, status):
    try:
        audio_queue.put_nowait(indata.copy())
    except queue.Full:
        pass


def prepare_audio_float32(audio_stereo: np.ndarray) -> np.ndarray:
    """
    audio_stereo: float32, shape (N, 2)
    returns: float32 mono 16kHz
    """
    # stereo -> mono
    mono = audio_stereo.mean(axis=1).astype(np.float32)

    # normalize (Ð·Ð°Ñ‰Ð¸Ñ‚Ð° Ð¾Ñ‚ ÐºÐ»Ð¸Ð¿Ð¿Ð¸Ð½Ð³Ð°/Ñ‚Ð¸Ñ…Ð¾Ð³Ð¾ ÑƒÑ€Ð¾Ð²Ð½Ñ)
    peak = float(np.max(np.abs(mono)) + 1e-9)
    mono = (mono / peak) * 0.9

    # resample 48k -> 16k
    audio_16k = resampy.resample(mono, INPUT_RATE, TARGET_RATE).astype(np.float32)
    return audio_16k


print("ðŸŽ§ Listening Voicemeeter B1... (Ctrl+C to stop)")
try:
    with sd.InputStream(
        device=DEVICE_ID,
        channels=CHANNELS,
        samplerate=INPUT_RATE,
        blocksize=BLOCKSIZE,
        dtype="float32",
        callback=callback
    ):
        while True:
            try:
                data = audio_queue.get(timeout=0.1)
            except queue.Empty:
                continue

            vol = rms(data)
            now = time.time()

            # ÑÑ‚Ð°Ñ€Ñ‚ Ñ€ÐµÑ‡Ð¸
            if vol > VOLUME_THRESHOLD:
                if not segment_active:
                    print("ðŸ“ž Speech detected")
                    segment_active = True
                    current_audio = []
                last_voice_time = now
                current_audio.append(data)

            # ÐºÐ¾Ð½ÐµÑ† Ñ€ÐµÑ‡Ð¸ Ð¿Ð¾ Ñ‚Ð¸ÑˆÐ¸Ð½Ðµ
            if segment_active and last_voice_time and (now - last_voice_time) > SILENCE_TIMEOUT:
                segment_active = False
                print("ðŸ“´ Silence â†’ transcribing")

                if not current_audio:
                    continue

                audio_np = np.concatenate(current_audio, axis=0)
                dur_in = len(audio_np) / INPUT_RATE
                if dur_in < MIN_AUDIO_SECONDS:
                    print("âš ï¸ Too short, skipping")
                    continue

                audio_16k = prepare_audio_float32(audio_np)
                dur_16k = len(audio_16k) / TARGET_RATE
                if dur_16k < MIN_AUDIO_SECONDS:
                    print("âš ï¸ Too short after resample, skipping")
                    continue

                segments, _ = model.transcribe(
                    audio_16k,
                    language=LANG,
                    initial_prompt=PROMPT,
                    beam_size=BEAM_SIZE,
                    vad_filter=VAD_FILTER,
                    condition_on_previous_text=True,
                )

                text = " ".join(seg.text.strip() for seg in segments).strip()

                if text:
                    fname = f"calls/call_{datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}.txt"
                    with open(fname, "w", encoding="utf-8") as f:
                        f.write(text)
                    print(f"ðŸ“ SAVED {fname}")
                    print("TEXT:", text)
                else:
                    print("âš ï¸ No text recognized")

except KeyboardInterrupt:
    print("\nðŸ›‘ Stopped by user")
    sys.exit(0)
